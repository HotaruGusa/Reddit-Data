{"cells":[{"cell_type":"markdown","source":["### Import Package"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53455e0a-d9ae-4ece-ad17-04a3abf61832","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# upgrade pip\n!pip install --upgrade pip"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"929871af-5991-4259-9125-f709084c0284","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# import libraries\nimport os\nimport pandas as pd\nimport re\nimport pyspark.sql.functions as f\nfrom pyspark.sql import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.functions import col,sum,avg,max,count\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport subprocess\nimport sys\nfrom collections import ChainMap\nimport plotly.graph_objects as go\nfrom pyspark.sql.functions import array_remove\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\nimport sparknlp\nfrom sparknlp.pretrained import PretrainedPipeline\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import collect_list\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nfrom pyspark.sql.types import ArrayType,StringType,IntegerType"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60670843-ffe9-42a1-a296-10faa93f44f9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Read data and clean rows"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e040095-269d-4bdc-83e2-5f0098d4a696","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# read from submission data\nnlp_df = spark.read.parquet(\"/FileStore/ML_data\")\nnlp_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4968dff0-bb89-41a6-99fc-80b670a78e5d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# count the nlp\nnlp_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31f5e35e-3069-4789-b9f5-ba763975b075","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# remove deleted and removed posts\nnlp_df = nlp_df.filter(col('selftext') != \"[deleted]\").filter(col('selftext') != \"[removed]\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60ab1047-0060-42ff-9238-679ad3c198f5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# remove Na\nnlp_df = nlp_df.filter(col('selftext') != '')\\\n.filter(~col('selftext').contains('None'))\\\n.filter(~col('selftext').contains('NULL'))\\\n.filter(~col('selftext').isNull())\\\n.filter(~isnan(col(\"selftext\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de594b45-d3e7-49c0-9860-33fea4c91e01","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# start pyspark\nspark = sparknlp.start()\nprint(\"Spark NLP version\", sparknlp.version())\nprint(\"Apache Spark version:\", spark.version)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"52bccf0a-588b-4ada-9319-d56e71baee79","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Pipeline to clean data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d43edad-86bb-4b1f-91a7-189dbcfeae19","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# add extra stopwords\nstopwords = list(STOPWORDS)\nmore_stop = ['mt', 'https', 'c', 'removed', 'deleted', '1', 'https:', '?','one', 'it', 'redd','2','&amp','x200b','HTTPS:','im','ive','dont','know','ampx200b','cant']\nstopwords += more_stop"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c91160b5-4070-41a1-8434-351a46aa8355","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# NLP pipeline\ndocument = DocumentAssembler()\\\n    .setInputCol(\"selftext\")\\\n    .setOutputCol(\"document\")\n\n# regex tokenizer\nregexTokenizer = RegexTokenizer() \\\n   .setInputCols([\"document\"]) \\\n   .setOutputCol(\"regexToken\") \\\n   .setToLowercase(True) \\\n   .setPattern(\"\\\\s+\")\n\n# normalizer\nnormalizer = Normalizer() \\\n    .setInputCols([\"regexToken\"]) \\\n    .setOutputCol(\"normalized\") \\\n    .setLowercase(True) \\\n    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"]) # remove punctuations (keep alphanumeric chars)\n\n# stopwords clean\nstop_words_1 = StopWordsCleaner()\\\n    .setInputCols(\"normalized\")\\\n    .setOutputCol(\"cleanTokens\")\\\n    .setStopWords(stopwords)\n\n# stemmer\nstemmer = Stemmer() \\\n    .setInputCols([\"cleanTokens\"]) \\\n    .setOutputCol(\"stem\")\n\n# stopwords clean\nstop_words_2 = StopWordsCleaner()\\\n    .setInputCols(\"stem\")\\\n    .setOutputCol(\"cleanTokens2\")\\\n    .setStopWords(stopwords)\n\n# lemmatizer\nlemmatizer =  LemmatizerModel.pretrained()\\\n    .setInputCols([\"cleanTokens2\"]) \\\n    .setOutputCol(\"lemma\") \\\n\n# integrate pipeline\nprediction_pipeline = Pipeline(\n    stages = [\n        document,\n        regexTokenizer,\n        normalizer,\n        stop_words_1,\n        stemmer,\n        stop_words_2,\n        lemmatizer\n    ]\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ee0d02e-ba7a-4274-af42-d32f4ab8cefb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# set selftext to go over the pipeline\nprediction_data = nlp_df.select('selftext')\nresult = prediction_pipeline.fit(prediction_data).transform(prediction_data)\nresult.select('lemma.result').show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5f3aa55-6101-4ba4-a211-a79406f45886","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### text distribution"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"804a8ead-5cf8-421a-84bf-44a8432f9714","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# show the results\nclean_result = result.select('cleanTokens.result')\nclean_result.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9dd1bf78-174b-4eca-95ea-b1b0fd4ad3d6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# get text length and how the results\nclean_result = clean_result.withColumn(\"text_length\",f.size(f.col(\"result\")))\nclean_result.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4f0b249-f792-40ad-8a29-d75486b12d33","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["clean_result = clean_result.withColumn(\"idx\", monotonically_increasing_id())\nnlp_df_2 = nlp_df.withColumn(\"idx\", monotonically_increasing_id())\n# join the dataframes\nnlp_df_2 = nlp_df_2.join(clean_result,[\"idx\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5689503e-8295-4dbb-be9e-ebbb66fd7c29","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nlp_df_2.write.parquet('/FileStore/ML_preparing')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b5ed9ff-31c8-4ad5-8324-00fca9de6486","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML_clean","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2354755412609163}},"nbformat":4,"nbformat_minor":0}
